{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('solution_ex_13').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "inputPath = './data'\n",
    "in_file_neigh = inputPath + '/neighbors.txt'\n",
    "reads_file = inputPath + '/readings.txt'\n",
    "\n",
    "outputPath_1 = './output_ex_13_reads'\n",
    "outputPath_2 = './output_ex_13_neigh'\n",
    "\n",
    "print(os.path.exists(inputPath))\n",
    "print(os.path.exists(outputPath_1))\n",
    "print(os.path.exists(outputPath_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "readRDD = spark.sparkContext.textFile(reads_file)\n",
    "treshFreeSlots = 3\n",
    "treshCritPerc = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticalSituation(line):\n",
    "    fields = line.split(\",\")\n",
    "    # fields[0] -> station id\n",
    "    # fields[5] -> free slots\n",
    "    stationId = fields[0]\n",
    "    numFreeSlots = int(fields[5])\n",
    "    \n",
    "    if  numFreeSlots < treshFreeSlots:\n",
    "        return (stationId, (1, 1))\n",
    "    else:\n",
    "        return (stationId, (1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s4', 1.0), ('s3', 0.4), ('s2', 0.25), ('s1', 0.2), ('s5', 0.2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each read/line map them to a read - situation rdd\n",
    "stationReads = readRDD.map(criticalSituation)\n",
    "\n",
    "# for each station compute the total of reads and the total of critical situations\n",
    "stationCounts = stationReads.reduceByKey(lambda c1, c2: (c1[0]+c2[0], c1[1]+c2[1]) )\n",
    "\n",
    "# compute percentages for station of critSituations\n",
    "stationCritSituation = stationCounts.mapValues(lambda counters: counters[1]/counters[0])\n",
    "\n",
    "# stations sorted\n",
    "selectedStationsSorted = stationCritSituation.sortBy(lambda sensorPerc: sensorPerc[1], ascending=False)\n",
    "\n",
    "selectedStationsSorted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selectedStationsSorted.saveAsTextFile(outputPath_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborsRDD = spark.sparkContext.textFile(in_file_neigh)\n",
    "\n",
    "# Map each line of the input file to a pair stationid, list of neighbor stations\n",
    "nPairRDD = neighborsRDD.map(lambda line: (line.split(\",\")[0], line.split(\",\")[1].split(\" \")) )\n",
    "\n",
    "# Create a local dictionary in the main memory of the driver that will be used to store the mapping \n",
    "# stationid -> list of neighbors\n",
    "# There are only 100 stations. Hence, you can suppose that data about neighbors can be stored in the main memory\n",
    "neighbors=nPairRDD.collectAsMap()\n",
    "\n",
    "# Select the lines/readings associated with a full status (number of free slots equal to 0)\n",
    "fullStatusLines = readRDD.filter(lambda line: int(line.split(\",\")[5])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTimestamp(reading):\n",
    "    fields = reading.split(\",\")\n",
    "    timestamp = fields[1] + fields[2] + fields[3]\n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of pairs with key = timestamp and value=reading associated with that timestamp\n",
    "# The concatenation of fields[1], fields[2], fields[3] is the timestamp of the reading\n",
    "fullLinesPRDD = fullStatusLines.map(lambda reading: (extractTimestamp(reading), reading))\n",
    "\n",
    "#  Collapse all the values with the same key in one single pair (timestamp, reading associated with that timestamp)\n",
    "fullReadingsPerTimestamp = fullLinesPRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectReadingssFunc(pairTimeStampListReadings):\n",
    "    # Extract the list of stations that appear in the readings\n",
    "    # associated with the current key \n",
    "    # (i.e., the list of stations that are full in this timestamp)\n",
    "    # The list of readings is in the value part of the inpput key-value pair\n",
    "    stations = []\n",
    "    for reading in pairTimeStampListReadings[1]:\n",
    "        # Extract the stationid from each reading\n",
    "        fields = reading.split(\",\")\n",
    "        stationId = fields[0]\n",
    "        stations.append(stationId)\n",
    "        \n",
    "        \n",
    "    # Iterate again over the list of readings to select the readings satistying the constraint on the \n",
    "    # full status situation of all neighboors \n",
    "    selectedReading = []\n",
    "\n",
    "    for reading in pairTimeStampListReadings[1]:\n",
    "        # This reading must be selected if all the neighbors of\n",
    "        # the station of this reading are also in the value of\n",
    "        # the current key-value pair (i.e., if they are in list stations)\n",
    "        # Extract the stationid of this reading\n",
    "        fields = reading.split(\",\")\n",
    "        stationId = fields[0]\n",
    "\n",
    "        # Select the list of neighbors of the current station\n",
    "        nCurrentStation = neighbors[stationId]\n",
    "        \n",
    "        # Check if all the neighbors of the current station are in value \n",
    "        # (i.e., the local list stations) of the current key-value pair\n",
    "        allNeighborsFull = True\n",
    "        \n",
    "        for neighborStation in nCurrentStation:\n",
    "            if neighborStation not in stations:\n",
    "                # There is at least one neighbor of th current station\n",
    "                # that is not in the full status in this timestamp\n",
    "                allNeighborsFull = False\n",
    "                \n",
    "        if allNeighborsFull == True:\n",
    "            selectedReading.append(reading)\n",
    "            \n",
    "    return selectedReading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each pair contains a timestamp and the list of readings (with number of free slots equal to 0) \n",
    "# associated with that timestamp.\n",
    "# Check, for each reading in the list, if all the neighbors of the station of that reading are \n",
    "# also present in this list of readings\n",
    "# Emit one \"string\" for each reading associated with a completely full status \n",
    "selectedReadingsRDD = fullReadingsPerTimestamp.flatMap(selectReadingssFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result in HDFS\n",
    "selectedReadingsRDD.saveAsTextFile(outputPath_2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
